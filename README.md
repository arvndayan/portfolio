# Aravind Reddy

**Data Engineer**

+1 (475)-666-8334 | reddyaravind743@gmail.com | Charlotte, US

## Summary

Data Engineer with 3 years of experience in developing data lake solutions, reducing data processing time by 40%, and enabling real-time analytics for business stakeholders. Collaborated with data scientists and business analysts to deliver impactful projects that drove revenue growth and operational efficiency. Experienced in big data tools and platforms like Databricks, Apache Spark, and Hadoop. Proficient in ETL, data integration, and data quality using tools such as SSIS, Alteryx, and AWS Glue.

## Skills

- **Methodologies:** SDLC, Agile, Waterfall
- **Languages:** Python, R, SQL, SAS
- **ML Algorithms:** Linear Regression, Logistic Regression, Supervised Learning, Unsupervised Learning, Classification, SVM, Random Forests, Naive Bayes, KNN, K Means
- **Packages:** NumPy, Pandas, Matplotlib, SciPy, Scikit-learn, TensorFlow
- **Visualization Tools:** Tableau, Power BI
- **Databases:** MySQL, Oracle, MS-SQL Server, HBase, Cassandra, DynamoDB, MongoDB
- **ETL Tools:** Informatica, Teradata
- **Big Data Technologies:** Hadoop (MapReduce, YARN, Hive, HBase, Flume, Sqoop), Spark (Core, SQL, Streaming), Hive, Kafka
- **Cloud Technologies:** AWS (S3, Glue, EC2, Lambda, Redshift), Azure (ML Studio, Data Factory, Databricks, Cosmos DB, Synapse Analytics, Data Lake), GCP (Cloud Dataflow, BigQuery, Pub/Sub)
- **Other Tools:** Git, MS Excel
- **Operating Systems:** Windows, Linux

## Experience

**Data Engineer**  
_Deutsche Bank, USA_  
_Aug 2023 - Current_

- Designed, developed, and maintained scalable data pipeline workflows to support Deutsche Bank's Tax Operations teams and other application teams within Tax technology.
- Partnered with business users and technology teams to understand data requirements, identify appropriate data sources, and design data pipelines for efficient ETL processes.
- Improved integration and productivity of the tax data platform by 30% through collaboration with other teams.
- Utilized AWS Glue for data integration and ETL process development, reducing ETL run times by 35%.
- Applied data integration tools like Python, SQL, SSIS, and Alteryx to improve data quality by 25% and decrease processing time by 40%.
- Enhanced data quality control measures, data validation procedures, and error control measures, reducing data errors by 20%.
- Implemented big data processing tools like Databricks, Apache Spark, and Hadoop to process large volumes of data more efficiently, enhancing data processing by 35%.
- Used AWS Cloud services such as AWS Key Management Service (KMS), Amazon S3, AWS Glue, and AWS Lambda to design and implement secure and scalable data storage solutions.

**Data Engineer**  
_Humana, USA_  
_Dec 2022 - Jul 2023_

- Collaborated with cross-functional teams to design and develop data pipelines and ETL workflows, ensuring efficient data extraction, transformation, and loading processes.
- Solved complex data integration issues using Python, SQL, and ETL tools like SSIS and Alteryx, enhancing data accuracy by 25% and reducing processing time by 20%.
- Worked with data scientists and analysts to identify data needs and delivered more efficient data structures, improving analytics and reporting by 30%.
- Implemented Azure Data Factory pipelines to automate ETL processes, reducing data processing time by 30%.
- Utilized big data processing frameworks like Apache Spark, Hadoop, and Databricks to manage and analyze datasets exceeding 1TB, reducing data treatment time by 50%.
- Maintained version control systems like Git and SVN to enhance code management and collaboration by 40%.
- Proficient in Jupyter, Eclipse, and Spyder IDEs for efficient data exploration, model development, and evaluation in the healthcare domain.
- Developed and deployed Azure Functions to streamline data processing tasks, enhancing operational efficiency and reducing manual intervention by 40%.
- Designed and deployed scalable and reliable data storage systems using Amazon S3, ensuring data integrity and availability.

**Junior Data Engineer**  
_Zensar Technologies, India_  
_Jul 2020 - Nov 2021_

- Generated insightful data visualizations using Python libraries such as Matplotlib, Seaborn, and Plotly, communicating complex information and trends to stakeholders.
- Executed data models, schemas, and database structures using MongoDB, MySQL, and Oracle technologies for data operations of datasets greater than 500GB.
- Established and fine-tuned data quality assurance procedures, data validation processes, and opportunities for improvement to enhance data credibility and minimize data discrepancies by 25%.
- Leveraged Azure SQL Database for efficient data storage and retrieval, ensuring high availability and performance for critical business applications.

## Education

**Master of Science in Data Science**  
_University of New Haven, USA_  
_May 2023_

**Bachelor of Engineering in Mechanical Engineering**  
_Sri Indu College of Engineering and Technology, India_  
_Jul 2021_

## Certifications

- Data Science Boot Camp (Udemy)
- Python Pro Boot Camp 2021 (Udemy)
- Statistics for Data Science and Business Analysis (Udemy)
