<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aravind Reddy's Portfolio</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Aravind Reddy</h1>
            <p>Data Engineer | Big Data Enthusiast</p>
            <p>ðŸ“« reddyaravind743@gmail.com | Charlotte, US</p>
        </div>
    </header>
    <nav>
        <ul>
            <li><a href="#about">About Me</a></li>
            <li><a href="#skills">Skills</a></li>
            <li><a href="#experience">Experience</a></li>
            <li><a href="#education">Education</a></li>
            <li><a href="#certifications">Certifications</a></li>
        </ul>
    </nav>
    <section id="about">
        <div class="container">
            <h2>About Me</h2>
            <p>I am a Data Engineer with over 3 years of experience in developing scalable data solutions, enhancing data processing efficiency, and enabling real-time analytics. I have a strong background in big data tools and platforms, including Databricks, Apache Spark, and Hadoop. I am proficient in ETL, data integration, and data quality.</p>
        </div>
    </section>
    <section id="skills">
        <div class="container">
            <h2>Skills</h2>
            <ul>
                <li><strong>Languages:</strong> Python, R, SQL, SAS</li>
                <li><strong>ML Algorithms:</strong> Linear Regression, Logistic Regression, Supervised Learning, Unsupervised Learning, Classification, SVM, Random Forests, Naive Bayes, KNN, K Means</li>
                <li><strong>Packages:</strong> NumPy, Pandas, Matplotlib, SciPy, Scikit-learn, TensorFlow</li>
                <li><strong>Visualization Tools:</strong> Tableau, Power BI</li>
                <li><strong>Databases:</strong> MySQL, Oracle, MS-SQL Server, HBase, Cassandra, DynamoDB, MongoDB</li>
                <li><strong>ETL Tools:</strong> SSIS, Alteryx, Informatica, Teradata</li>
                <li><strong>Big Data Technologies:</strong> Hadoop (MapReduce, YARN, Hive, HBase, Flume, Sqoop), Spark (Core, SQL, Streaming), Hive, Kafka</li>
                <li><strong>Cloud Technologies:</strong> AWS (S3, Glue, EC2, Lambda, Redshift), Azure (ML Studio, Data Factory, Databricks, Cosmos DB, Synapse Analytics, Data Lake), GCP (Cloud Dataflow, BigQuery, Pub/Sub)</li>
                <li><strong>Other Tools:</strong> Git, MS Excel</li>
                <li><strong>Operating Systems:</strong> Windows, Linux</li>
            </ul>
        </div>
    </section>
    <section id="experience">
        <div class="container">
            <h2>Experience</h2>
            <div class="job">
                <h3>Data Engineer - Deutsche Bank, USA</h3>
                <p><em>Aug 2023 - Current</em></p>
                <ul>
                    <li>Designed and maintained scalable data pipeline workflows to support Deutsche Bank's Tax Operations teams.</li>
                    <li>Partnered with business users and technology teams to understand data requirements and design data pipelines for efficient ETL processes.</li>
                    <li>Improved integration and productivity of the tax data platform by 30% through collaboration with other teams.</li>
                    <li>Utilized AWS Glue for data integration and ETL process development, reducing ETL run times by 35%.</li>
                    <li>Applied data integration tools like Python, SQL, SSIS, and Alteryx to improve data quality by 25% and decrease processing time by 40%.</li>
                    <li>Enhanced data quality control measures, data validation procedures, and error control measures, reducing data errors by 20%.</li>
                    <li>Implemented big data processing tools like Databricks, Apache Spark, and Hadoop to process large volumes of data more efficiently, enhancing data processing by 35%.</li>
                    <li>Used AWS Cloud services such as AWS Key Management Service (KMS), Amazon S3, AWS Glue, and AWS Lambda to design and implement secure and scalable data storage solutions.</li>
                </ul>
            </div>
            <div class="job">
                <h3>Data Engineer - Humana, USA</h3>
                <p><em>Dec 2022 - Jul 2023</em></p>
                <ul>
                    <li>Collaborated with cross-functional teams to design and develop data pipelines and ETL workflows, ensuring efficient data extraction, transformation, and loading processes.</li>
                    <li>Solved complex data integration issues using Python, SQL, and ETL tools like SSIS and Alteryx, enhancing data accuracy by 25% and reducing processing time by 20%.</li>
                    <li>Worked with data scientists and analysts to identify data needs and delivered more efficient data structures, improving analytics and reporting by 30%.</li>
                    <li>Implemented Azure Data Factory pipelines to automate ETL processes, reducing data processing time by 30%.</li>
                    <li>Utilized big data processing frameworks like Apache Spark, Hadoop, and Databricks to manage and analyze datasets exceeding 1TB, reducing data treatment time by 50%.</li>
                    <li>Maintained version control systems like Git and SVN to enhance code management and collaboration by 40%.</li>
                    <li>Proficient in Jupyter, Eclipse, and Spyder IDEs for efficient data exploration, model development, and evaluation in the healthcare domain.</li>
                    <li>Developed and deployed Azure Functions to streamline data processing tasks, enhancing operational efficiency and reducing manual intervention by 40%.</li>
                    <li>Designed and deployed scalable and reliable data storage systems using Amazon S3, ensuring data integrity and availability.</li>
                </ul>
            </div>
            <div class="job">
                <h3>Junior Data Engineer - Zensar Technologies, India</h3>
                <p><em>Jul 2020 - Nov 2021</em></p>
                <ul>
                    <li>Generated insightful data visualizations using Python libraries such as Matplotlib, Seaborn, and Plotly, communicating complex information and trends to stakeholders.</li>
                    <li>Executed data models, schemas, and database structures using MongoDB, MySQL, and Oracle technologies for data operations of datasets greater than 500GB.</li>
                    <li>Established and fine-tuned data quality assurance procedures, data validation processes, and opportunities for improvement to enhance data credibility and minimize data discrepancies by 25%.</li>
                    <li>Leveraged Azure SQL Database for efficient data storage and retrieval, ensuring high availability and performance for critical business applications.</li>
                </ul>
            </div>
        </div>
    </section>
    <section id="education">
        <div class="container">
            <h2>Education</h2>
            <p><strong>Master of Science in Data Science</strong><br>University of New Haven, USA - May 2023</p>
            <p><strong>Bachelor of Engineering in Mechanical Engineering</strong><br>Sri Indu College of Engineering and Technology, India - Jul 2021</p>
        </div>
    </section>
    <section id="certifications">
        <div class="container">
            <h2>Certifications</h2>
            <ul>
                <li>Data Science Boot Camp (Udemy)</li>
                <li>Python Pro Boot Camp 2021 (Udemy)</li>
                <li>Statistics for Data Science and Business Analysis (Udemy)</li>
            </ul>
        </div>
    </section>
    <footer>
        <div class="container">
            <p>&copy; 2024 Aravind Reddy. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
